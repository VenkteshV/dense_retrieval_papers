# dense_retrieval_papers


# Bi-encoders with hard negative sampling (with theoretical bounds)
1.  Efficient Training of Retrieval Models Using Negative Cache <a>https://openreview.net/pdf?id=824xC-SgWgU</a>
2.  Adversarial text ranker and re-ranker <a> https://openreview.net/pdf?id=MR7XubKUFB </a>
3.  Few shot learnign with siamese networks and label tuning <a> https://arxiv.org/pdf/2203.14655.pdf </a>
4.  ANCE APPROXIMATE NEAREST NEIGHBOR NEGATIVE CONTRASTIVE LEARNING FOR DENSE TEXT RETRIEVALg  <a> https://openreview.net/pdf?id=zeFrfgyZln </a> 
5.  ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval <a> https://arxiv.org/abs/2205.09153 </a>
6.  a pre-training architecture for dense retrieval http://www.cs.cmu.edu/~callan/Papers/emnlp21-Luyu_Gao.pdf
7.  Counterfactual hard negative sampling: https://arxiv.org/abs/2207.00148 
8.  Revisiting dense retrieval with unanswerable counterfactuals https://arxiv.org/pdf/2304.03031.pdf
# Review
1. [Semantic Models for the First-stage Retrieval: A Comprehensive Review](https://arxiv.org/abs/2103.04831)
# Unsupervised keyphrase extraction
1.  MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction <a>https://arxiv.org/pdf/2110.06651.pdf</a>


#  Question Answering
1.  Cross attention based question answering <a> https://aclanthology.org/P17-1021.pdf </a>
# Language model pre-training
1. LinkBERT <a> https://arxiv.org/pdf/2203.15827.pdf </a>

# blogs
1. Advances in ubiquitous information retrieval (https://ai.facebook.com/blog/-advances-toward-ubiquitous-neural-information-retrieval/)

